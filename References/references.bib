@article{Vinyals:15,
    author  = {Orial Vinyals and Quoc V. Le},
    title   = {A Neural Conversation Model},
    journal = {ICML Workshop on Deep Learning},
    year    = "2015"
}

@InProceedings{Jiwei:16,
  author    = {Li, Jiwei  and  Galley, Michel  and  Brockett, Chris  and  Gao, Jianfeng  and  Dolan, Bill},
  title     = {A Diversity-Promoting Objective Function for Neural Conversation Models},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {110--119},
  url       = {http://www.aclweb.org/anthology/N16-1014}
}

@inproceedings{Serban:16,
 author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
 title = {Building End-to-end Dialogue Systems Using Generative Hierarchical Neural Network Models},
 booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 series = {AAAI'16},
 year = {2016},
 location = {Phoenix, Arizona},
 pages = {3776--3783},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3016387.3016435},
 acmid = {3016435},
 publisher = {AAAI Press},
} 

@inproceedings{Kingma:14,
    author  = {Diederik P. Kingma and Max Welling},
    title   = {Auto-encoding Variational {Bayes}},
    booktitle = {International Conference on Learning Representations},
    year    = "2014"
}

@InProceedings{Rezende:14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@InProceedings{Titsias:14,
  title = 	 {Doubly Stochastic Variational Bayes for non-Conjugate Inference},
  author = 	 {Michalis Titsias and Miguel Lázaro-Gredilla},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1971--1979},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/titsias14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/titsias14.html},
  abstract = 	 {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.}
}

@incollection{Kingma:14b,
title = {Semi-supervised Learning with Deep Generative Models},
author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3581--3589},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf}
}


@incollection{Sohn:15,
title = {Learning Structured Output Representation using Deep Conditional Generative Models},
author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3483--3491},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf}
}




@InProceedings{Gregor:15,
  title = 	 {{DRAW}: {A} Recurrent Neural Network For Image Generation},
  author = 	 {Karol Gregor and Ivo Danihelka and Alex Graves and Danilo Rezende and Daan Wierstra},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1462--1471},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/gregor15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/gregor15.html},
  abstract = 	 {This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye.}
}

@inproceedings{Sundermeyer:12,
  title={LSTM neural networks for language modeling},
  author={Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={Thirteenth annual conference of the international speech communication association},
  year={2012}
}

@article{Mansimov:16,
  author    = {Elman Mansimov and
               Emilio Parisotto and
               Lei Jimmy Ba and
               Ruslan Salakhutdinov},
  title     = {Generating Images from Captions with Attention},
  journal   = {ICLR},
  year      = {2016}
}

@InProceedings{Bowman:16,
  author    = {Bowman, Samuel R.  and  Vilnis, Luke  and  Vinyals, Oriol  and  Dai, Andrew  and  Jozefowicz, Rafal  and  Bengio, Samy},
  title     = {Generating Sentences from a Continuous Space},
  booktitle = {Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {10--21},
  url       = {http://www.aclweb.org/anthology/K16-1002}
}


@article{Zhang:16,
  author    = {Biao Zhang and
               Deyi Xiong and
               Jinsong Su},
  title     = {Variational Neural Machine Translation},
  journal   = {EMNLP},
  year      = {2016}
}

@article{Miao:16a,
  title = "Neural Variational Inference for Text Processing",
  author = "Yishu Miao and Lei Yu and Phil Blunsom",
  year = "2016",
  journal = "ICML"
}

@article{Miao:16b,
  title = "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
  author = "Yishu Miao and Phil Blunsom",
  year = "2016",
  journal = "EMNLP"
}


@InProceedings{Jiwei:16b,
  author    = {Li, Jiwei  and  Galley, Michel  and  Brockett, Chris  and  Spithourakis, Georgios  and  Gao, Jianfeng  and  Dolan, Bill},
  title     = {A Persona-Based Neural Conversation Model},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {994--1003},
  url       = {http://www.aclweb.org/anthology/P16-1094}
}



@article{GRU,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               Dzmitry Bahdanau and
               Yoshua Bengio},
  title     = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  journal   = {Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  year      = {2014},
}

@InProceedings{Shang:15,
  author    = {Shang, Lifeng  and  Lu, Zhengdong  and  Li, Hang},
  title     = {Neural Responding Machine for Short-Text Conversation},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = {July},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {1577--1586},
  url       = {http://www.aclweb.org/anthology/P15-1152}
}


@article{Sordoni:15,
  author    = {Sordoni, Alessandro  and  Galley, Michel  and  Auli, Michael  and  Brockett, Chris  and  Ji, Yangfeng  and  Mitchell, Margaret  and  Nie, Jian-Yun  and  Gao, Jianfeng  and  Dolan, Bill},
  title     = {A Neural Network Approach to Context-Sensitive Generation of Conversational Responses},
  journal = {NAACL},
  year = 2015
}

@InProceedings{Ritter:11,
  author    = {Ritter, Alan  and  Cherry, Colin  and  Dolan, William B.},
  title     = {Data-Driven Response Generation in Social Media},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  month     = {July},
  year      = {2011},
  address   = {Edinburgh, Scotland, UK.},
  publisher = {Association for Computational Linguistics},
  pages     = {583--593},
  url       = {http://www.aclweb.org/anthology/D11-1054}
}

@article{Sutskever:14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {NIPS},
  year      = {2014}
}

@InProceedings{Liu:2016,
  author    = {Liu, Chia-Wei  and  Lowe, Ryan  and  Serban, Iulian  and  Noseworthy, Mike  and  Charlin, Laurent  and  Pineau, Joelle},
  title     = {How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {2122--2132},
  url       = {https://aclweb.org/anthology/D16-1230}
}



@article{Mitchell:2015,
author = {David Mitchell},
title = {Type-token models: a comparative study},
journal = {Journal of Quantitative Linguistics},
year = {2015},
}



@article{Morin:05,
  author    = {Frederic Morin and Yoshua Bengio},
  title     = {Hierarchical Probabilistic Neural Network Language Model},
  journal   = {Tenth International Workshop on Artificial Intelligence and Statistics},
  year      = {2005},
}


@inproceedings{Tiedeman:12,
    author = {J{\"o}rg Tiedemann},
    url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf},
    note = {ACL Anthology Identifier: L12-1246},
    title = {Parallel Data, Tools and Interfaces in OPUS},
    booktitle = {Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)},
    year = {2012},
    month = {May},
    address = {Istanbul, Turkey},
    editor = {Nicoletta Calzolari and Khalid Choukri and Thierry Declerck and Mehmet U\u{g}ur Do\u{g}an and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
    publisher = {European Language Resources Association (ELRA)},
    isbn = {978-2-9517408-7-7},
    language = {English},
    pages = {2214--2218}
    }



@article{Zeiler:12,
  author    = {Matthew D. Zeiler},
  title     = {{ADADELTA:} An Adaptive Learning Rate Method},
  journal   = {CoRR},
  volume    = {abs/1212.5701},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.5701},
}

@inproceedings{Tu:2017,
  author = {Zhaopeng Tu and 
            Yang Liu and
            Lifeng Shang and
            Xiaohua Liu and
            Yang Liu},
  title = {Neural Machine Translation with Reconstruction},
  booktitle = {AAAI},
  year      = {2017},
}

@InProceedings{Belz:2007,
  author    = {Belz, Anja},
  title     = {Probabilistic Generation of Weather Forecast Texts},
  booktitle = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference},
  month     = {April},
  year      = {2007},
  address   = {Rochester, New York},
  publisher = {Association for Computational Linguistics},
  pages     = {164--171},
  url       = {http://www.aclweb.org/anthology/N/N07/N07-1021}
}



@inproceedings{Serban:17,
  author = {Iulian Vlad Serban and Alessandro Sordoni and Ryan Lowe and Laurent Charlin and
            Joelle Pineau and Aaron Courville and Yoshua Bengio},
  title = {A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues},
  booktitle = {AAAI},
  year = {2017},
}
  

@misc{keras,
  author = {Chollet, François},
  title = {Keras},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/fchollet/keras}}
}


@article{theano1,
  author    = {{Theano Development Team}},
  title     = {Theano: {A} {Python} framework for fast computation of mathematical
               expressions},
  journal   = {arXiv preprints},
  volume    = {abs/1605.02688},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.02688},
  timestamp = {Wed, 28 Dec 2016 11:28:29 +0100},
}

@article{Weizenbaum:66,
 author = {Weizenbaum, Joseph},
 title = {ELIZA\&Mdash;a Computer Program for the Study of Natural Language Communication Between Man and Machine},
 journal = {Commun. ACM},
 issue_date = {Jan. 1966},
 volume = {9},
 number = {1},
 month = jan,
 year = {1966},
 issn = {0001-0782},
 pages = {36--45},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/365153.365168},
 doi = {10.1145/365153.365168},
 acmid = {365168},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@book{Weizenbaum:76,
 author = {Weizenbaum, Joseph},
 title = {Computer Power and Human Reason: From Judgment to Calculation},
 year = {1976},
 isbn = {0716704641},
 publisher = {W. H. Freeman \& Co.},
 address = {New York, NY, USA},
}

@article{Turing:50,
author = {Turing, A. M.},
title = {Computing Machinery and Intelligence},
journal = {Mind},
volume = {LIX},
number = {236},
pages = {433-460},
year = {1950},
doi = {10.1093/mind/LIX.236.433},
URL = {http://dx.doi.org/10.1093/mind/LIX.236.433},
eprint = {/oup/backfile/content_public/journal/mind/lix/236/10.1093_mind_lix.236.433/1/433.pdf}
}

@Article{Marcus:93,
  author = 	"Marcus, Mitchell P.
		and Santorini, Beatrice
		and Marcinkiewicz, Mary Ann",
  title = 	"Building a Large Annotated Corpus of English: The Penn Treebank",
  journal = 	"Computational Linguistics",
  year = 	"1993",
  volume = 	"19",
  number = 	"2",
  url = 	"http://aclweb.org/anthology/J93-2004"
}

@inproceedings{Deng:09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@incollection{Krizhevsky:12,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{Wu:16,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}

@Article{Jelinek:05,
author="Jelinek, Frederick",
title="Some of my Best Friends are Linguists",
journal="Language Resources and Evaluation",
year="2005",
month="Feb",
day="01",
volume="39",
number="1",
pages="25--34",
issn="1572-0218",
doi="10.1007/s10579-005-2693-4",
url="https://doi.org/10.1007/s10579-005-2693-4"
}

@InProceedings{Martinez-Alonso:16,
  author = 	"Mart{\'i}nez Alonso, H{\'e}ctor
		and Seddah, Djam{\'e}
		and Sagot, Beno{\^i}t",
  title = 	"From Noisy Questions to Minecraft Texts: Annotation Challenges in Extreme      Syntax Scenario    ",
  booktitle = 	"Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"13--23",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/W16-3905"
}

@Article{Pritchard:00,
author={Pritchard, J. K.
and Stephens, M.
and Donnelly, P.},
title={Inference of population structure using multilocus genotype data},
journal={Genetics},
year={2000},
month={Jun},
volume={155},
number={2},
pages={945-959},
abstract={We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
note={10835412[pmid]},
note={PMC1461096[pmcid]},
issn={0016-6731},
url={https://www.ncbi.nlm.nih.gov/pubmed/10835412}
}

@InProceedings{Pang:02,
  author    = {Bo Pang  and  Lillian Lee  and  Shivakumar Vaithyanathan},
  title     = {Thumbs up? Sentiment Classification using Machine Learning Techniques},
  booktitle = {Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing},
  month     = {July},
  year      = {2002},
  publisher = {Association for Computational Linguistics},
  pages     = {79--86},
  url       = {http://www.aclweb.org/anthology/W02-1011},
  doi       = {10.3115/1118693.1118704}
}

@InProceedings{Zhang:14,
  author    = {Zhang, Xingxing  and  Lapata, Mirella},
  title     = {Chinese Poetry Generation with Recurrent Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = {October},
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  pages     = {670--680},
  url       = {http://www.aclweb.org/anthology/D14-1074}
}

@Book{Plato:52,
author={Plato},
title={Plato\'s Phaedrus},
year={1952},
publisher={Cambridge University Press, 1952.},
abstract={vii, 172 pages ; 23 cm},
note={Includes index.;Bibliographical footnotes.},
url={https://search.library.wisc.edu/catalog/999894420902121}
}

@inproceedings{Sahami:98,
  added-at = {2007-07-23T14:56:58.000+0200},
  address = {Madison, Wisconsin},
  author = {Sahami, Mehran and Dumais, Susan and Heckerman, David and Horvitz, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/228e6c4a1df2d6c8d2e78498275248d30/zeno},
  booktitle = {Learning for Text Categorization: Papers from the 1998 Workshop},
  ee = {ftp://ftp.research.microsoft.com/pub/ejh/junkfilter.pdf},
  interhash = {190c5ba286aa1cd361334bb9edbc36cc},
  intrahash = {28e6c4a1df2d6c8d2e78498275248d30},
  keywords = {1998 ais-07w email first-paper naive-bayes spam text-classification},
  publisher = {AAAI Technical Report WS-98-05},
  timestamp = {2007-07-25T15:20:03.000+0200},
  title = {A Bayesian Approach to Filtering Junk {E}-Mail},
  url = {citeseer.ist.psu.edu/sahami98bayesian.html},
  year = 1998
}

@INPROCEEDINGS{Pantel:98,
    author = {Patrick Pantel and Dekang Lin},
    title = {SpamCop: A Spam Classification \& Organization Program},
    booktitle = {Learning for Text Categorization: Papers from the 1998 Workshop},
    year = {1998},
    pages = {95--98}
}

@ARTICLE{Blei:03,
  AUTHOR = {Blei, D. and Ng, A. and Jordan, M.},
  TITLE = {Latent {D}irichlet Allocation},
  JOURNAL = {Journal of Machine Learning Research},
  VOLUME = 3,
  PAGES = {993-1022},
  URL = {http://www.cs.berkeley.edu/~blei/papers/blei03a.ps.gz},
  MONTH = JAN,
  YEAR = 2003
}

@InProceedings{Hopkins:17,
  author = 	"Hopkins, Jack
		and Kiela, Douwe",
  title = 	"Automatically Generating Rhythmic Verse with Neural Networks",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"168--178",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-1016",
  url = 	"http://aclweb.org/anthology/P17-1016"
}

@InProceedings{Ghazvininejad:16,
  author    = {Ghazvininejad, Marjan  and  Shi, Xing  and  Choi, Yejin  and  Knight, Kevin},
  title     = {Generating Topical Poetry},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1183--1191},
  url       = {https://aclweb.org/anthology/D16-1126}
}

@inproceedings{Zhu:17,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

@Article{Tipping:99,
author = {Tipping, M. E. and Bishop, Christopher},
title = {Probabilistic Principal Component Analysis},
year = {1999},
month = {January},
abstract = {

Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.


},
url = {https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis/},
pages = {611-622},
journal = {Journal of the Royal Statistical Society, Series B},
volume = {21},
number = {3},
note = {Available from  http://www.ncrg.aston.ac.uk/Papers/index.html},
}

@inproceedings{Hofmann:99,
 author = {Hofmann, Thomas},
 title = {Probabilistic Latent Semantic Analysis},
 booktitle = {Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'99},
 year = {1999},
 isbn = {1-55860-614-9},
 location = {Stockholm, Sweden},
 pages = {289--296},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2073796.2073829},
 acmid = {2073829},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@InProceedings{Klein:04,
  author = 	"Klein, Dan
		and Manning, Christopher",
  title = 	"Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency",
  booktitle = 	"Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)",
  year = 	"2004",
  url = 	"http://aclweb.org/anthology/P04-1061"
}

@InProceedings{Blunsom:10,
  author    = {Blunsom, Phil  and  Cohn, Trevor},
  title     = {Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  month     = {October},
  year      = {2010},
  address   = {Cambridge, MA},
  publisher = {Association for Computational Linguistics},
  pages     = {1204--1213},
  url       = {http://www.aclweb.org/anthology/D10-1117}
}

@ARTICLE{Dempster:77,
    author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
    title = {Maximum likelihood from incomplete data via the EM algorithm},
    journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
    year = {1977},
    volume = {39},
    number = {1},
    pages = {1--38}
}

@InProceedings{Koehn:17,
  author    = {Koehn, Philipp  and  Knowles, Rebecca},
  title     = {Six Challenges for Neural Machine Translation},
  booktitle = {Proceedings of the First Workshop on Neural Machine Translation},
  month     = {August},
  year      = {2017},
  address   = {Vancouver},
  publisher = {Association for Computational Linguistics},
  pages     = {28--39},
  abstract  = {We explore six challenges for neural machine translation: domain mismatch,
	amount of training data, rare words, long sentences, word alignment, and beam
	search. We show both deficiencies and improvements over the quality of
	phrase-based statistical machine translation.},
  url       = {http://www.aclweb.org/anthology/W17-3204}
}

@Article{Cybenko:89,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
year="1989",
month="Dec",
day="01",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="https://doi.org/10.1007/BF02551274"
}

@ARTICLE{Shannon:51,
author={C. E. Shannon},
journal={The Bell System Technical Journal},
title={Prediction and entropy of printed English},
year={1951},
volume={30},
number={1},
pages={50-64},
keywords={},
doi={10.1002/j.1538-7305.1951.tb01366.x},
ISSN={0005-8580},
month={Jan},}

@ARTICLE{Jelinek:76,
author={F. Jelinek},
journal={Proceedings of the IEEE},
title={Continuous speech recognition by statistical methods},
year={1976},
volume={64},
number={4},
pages={532-556},
keywords={Speech recognition;Statistical analysis;Speech processing;Loudspeakers;Natural languages;Statistics;Signal processing;Automatic speech recognition;Decoding;Acoustic devices},
doi={10.1109/PROC.1976.10159},
ISSN={0018-9219},
month={April},}

@ARTICLE{Chomsky:56,
author={N. Chomsky},
journal={IRE Transactions on Information Theory},
title={Three models for the description of language},
year={1956},
volume={2},
number={3},
pages={113-124},
keywords={Languages;Markov processes;Natural languages;Testing;Laboratories;Markov processes;Impedance matching;Kernel;Research and development},
doi={10.1109/TIT.1956.1056813},
ISSN={0096-1000},
month={Sep.},}

@article{Elman:90,
author = {Elman, Jeffrey L.},
title = {Finding Structure in Time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
doi = {10.1207/s15516709cog1402\_1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
year = {1990}
}

@article{Hochreiter:97,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
title = {Long Short-Term Memory},
journal = {Neural Computation},
volume = {9},
number = {8},
pages = {1735-1780},
year = {1997},
doi = {10.1162/neco.1997.9.8.1735},

URL = { 
        https://doi.org/10.1162/neco.1997.9.8.1735
    
},
eprint = { 
        https://doi.org/10.1162/neco.1997.9.8.1735
    
}
,
    abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}

@misc{Chelba:17,
title	= {Language Modeling in the Era of Abundant Data},
author	= {Ciprian Chelba},
year	= {2017}
}

@article{Ackley:85,
title = "A learning algorithm for boltzmann machines",
journal = "Cognitive Science",
volume = "9",
number = "1",
pages = "147 - 169",
year = "1985",
issn = "0364-0213",
doi = "https://doi.org/10.1016/S0364-0213(85)80012-4",
url = "http://www.sciencedirect.com/science/article/pii/S0364021385800124",
author = "David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski",
abstract = "The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure."
}

@article{Dayan:95,
author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
title = {The Helmholtz Machine},
journal = {Neural Computation},
volume = {7},
number = {5},
pages = {889-904},
year = {1995},
doi = {10.1162/neco.1995.7.5.889},

URL = { 
        https://doi.org/10.1162/neco.1995.7.5.889
    
},
eprint = { 
        https://doi.org/10.1162/neco.1995.7.5.889
    
}
,
    abstract = { Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways. }
}

@InProceedings{Cao:16,
  author    = {Cao, Kris  and  Rei, Marek},
  title     = {A Joint Model for Word Embedding and Word Morphology},
  booktitle = {Proceedings of the 1st Workshop on Representation Learning for NLP},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {18--26},
  url       = {http://anthology.aclweb.org/W16-1603}
}

@incollection{Mikolov:13,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{Cao:18,
title={Emergent Communication through Negotiation},
author={Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk6WhagRW},
}

@article{Frost:98,
  title={Neuroscience, Play, and Child Development.},
  author={Frost, Joe L},
  year={1998},
  publisher={ERIC}
}

@incollection{Kohl:18,
title = {A Probabilistic U-Net for Segmentation of Ambiguous Images},
author = {Kohl, Simon and Romera-Paredes, Bernardino and Meyer, Clemens and De Fauw, Jeffrey and Ledsam, Joseph R. and Maier-Hein, Klaus and Eslami, S. M. Ali and Jimenez Rezende, Danilo and Ronneberger, Olaf},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6965--6975},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7928-a-probabilistic-u-net-for-segmentation-of-ambiguous-images.pdf}
}

@incollection{Baker:90,
 author = {Baker, James K.},
 chapter = {Stochastic Modeling for Automatic Speech Understanding},
 title = {Readings in Speech Recognition},
 editor = {Waibel, Alex and Lee, Kai-Fu},
 year = {1990},
 isbn = {1-55860-124-4},
 pages = {297--307},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=108235.108255},
 acmid = {108255},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@inproceedings{Melis:18,
title={On the State of the Art of Evaluation in Neural Language Models},
author={Gábor Melis and Chris Dyer and Phil Blunsom},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ByJHuTgA-},
}

@inproceedings{Mikolov:10,
  author = {Mikolov, Tomas and Karafi\'at, Martin and Burget, Luk\'a\v{s} and \v{C}ernock\'y, Jan and Khudanpur, Sanjeev},
  biburl = {https://www.bibsonomy.org/bibtex/2aee1e280d06e82474b17c4996aaea076/dblp},
  booktitle = {INTERSPEECH},
  editor = {Kobayashi, Takao and Hirose, Keikichi and Nakamura, Satoshi},
  ee = {http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html},
  keywords = {dblp},
  pages = {1045-1048},
  publisher = {ISCA},
  timestamp = {2015-06-21T06:10:05.000+0200},
  title = {Recurrent neural network based language model.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10},
  year = 2010
}

@inproceedings{Hinton:83,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
  keywords = {nn},
  priority = {2},
  timestamp = {2008-03-11T15:03:55.000+0100},
  title = {Optimal Perceptual Inference},
  year = 1983
}

@incollection{Smolensky:86,
 author = {Smolensky, P.},
 chapter = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {194--281},
 numpages = {88},
 url = {http://dl.acm.org/citation.cfm?id=104279.104290},
 acmid = {104290},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Article{Gorur:10,
author="G{\"o}r{\"u}r, Dilan
and Edward Rasmussen, Carl",
title="Dirichlet Process Gaussian Mixture Models: Choice of the Base Distribution",
journal="Journal of Computer Science and Technology",
year="2010",
month="Jul",
day="01",
volume="25",
number="4",
pages="653--664",
abstract="In the Bayesian mixture modeling framework it is possible to infer the necessary number of components to model the data and therefore it is unnecessary to explicitly restrict the number of components. Nonparametric mixture models sidestep the problem of finding the ``correct'' number of mixture components by assuming infinitely many components. In this paper Dirichlet process mixture (DPM) models are cast as infinite mixture models and inference using Markov chain Monte Carlo is described. The specification of the priors on the model parameters is often guided by mathematical and practical convenience. The primary goal of this paper is to compare the choice of conjugate and non-conjugate base distributions on a particular class of DPM models which is widely used in applications, the Dirichlet process Gaussian mixture model (DPGMM). We compare computational efficiency and modeling performance of DPGMM defined using a conjugate and a conditionally conjugate base distribution. We show that better density models can result from using a wider class of priors with no or only a modest increase in computational effort.",
issn="1860-4749",
doi="10.1007/s11390-010-9355-8",
url="https://doi.org/10.1007/s11390-010-9355-8"
}

@article{Hinton:02,
 author = {Hinton, Geoffrey E.},
 title = {Training Products of Experts by Minimizing Contrastive Divergence},
 journal = {Neural Comput.},
 issue_date = {August 2002},
 volume = {14},
 number = {8},
 month = aug,
 year = {2002},
 issn = {0899-7667},
 pages = {1771--1800},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760128018},
 doi = {10.1162/089976602760128018},
 acmid = {639730},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Tieleman:08,
    author = {Tijmen Tieleman},
    title = {Training restricted Boltzmann machines using approximations to the likelihood gradient},
    booktitle = {Proceedings of the 25th international conference on Machine learning},
    year = {2008},
    pages = {1064--1071}
}

@article{Hinton:95,
	author = {Hinton, GE and Dayan, P and Frey, BJ and Neal, RM},
	title = {The "wake-sleep" algorithm for unsupervised neural networks},
	volume = {268},
	number = {5214},
	pages = {1158--1161},
	year = {1995},
	doi = {10.1126/science.7761831},
	publisher = {American Association for the Advancement of Science},
	abstract = {An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up "recognition" connections convert the input into representations in successive hidden layers, and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above. In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the "sleep" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.},
	issn = {0036-8075},
	URL = {http://scientce.sciencemag.org/content/268/5214/1158},
	eprint = {http://science.sciencemag.org/content/268/5214/1158.full.pdf},
	journal = {Science}
}

@incollection{Hinton:09,
title = {Replicated Softmax: an Undirected Topic Model},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
pages = {1607--1614},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3856-replicated-softmax-an-undirected-topic-model.pdf}
}

@article{Hornik:91,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives."
}